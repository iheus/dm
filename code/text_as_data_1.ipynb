{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text as Data using Python\n",
    "By Shuhei Kitamura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "1. Introduction\n",
    "2. Import data\n",
    "3. Clean data\n",
    "4. Vectorization\n",
    "5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Introduction\n",
    "- Nowadays, many kinds of text data are available online.\n",
    "    - E.g., news articles, customer reviews, tweets, etc.\n",
    "- Possible analysis using such data includes:\n",
    "    - Sentiment analysis (pos/neg), conflict/hate speech detection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's evaluate the sentiment of user reviews in [IMDb (Internet Movie Database)](https://www.imdb.com/)!\n",
    "    - You can possibly apply the same method to analyze the sentiment of e.g. Amazon customer reviews.\n",
    "- In particular, we:\n",
    "    1. Prepare data for analysis using techniques in natural language processing (NLP)\n",
    "    2. Convert words to a matrix\n",
    "    3. Conduct sentiment analysis using machine learning\n",
    "- The following packages will be mainly used:\n",
    "    - nltk\n",
    "    - scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk; nltk.download('stopwords'); nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('...') # set the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Import data\n",
    "- Let's import data.\n",
    "    - ``full_train.txt``: train data.\n",
    "    - ``full_test.txt``: test data\n",
    "- We will train the machine using ``full_train.txt`` and then apply it to ``full_test.txt``.\n",
    "- All data were taken from [this website](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train = [] # import data as a list\n",
    "with open('data/full_train.txt', encoding=\"utf-8\") as file: \n",
    "    for line in file:\n",
    "        train.append(line.strip())\n",
    "        \n",
    "test = [] # import data as a list\n",
    "with open('data/full_test.txt', encoding=\"utf-8\") as file: \n",
    "    for line in file:\n",
    "        test.append(line.strip())\n",
    "        \n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print train data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Clean data\n",
    "### - Remove unnecessary items\n",
    "- The text contains many unnecessary items such as punctuation marks (., :, ;, !, ?), parentheses, etc.\n",
    "- Let's remove them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def clean_texts(text):\n",
    "    text = [re.compile(\"[*.;:!\\'?,\\\"()\\[\\]]\").sub(\"\", line.lower()) for line in text] # make all text lowe case, then replace punctuation marks, parantheses, etc. by \"\" (e.g., \"isn't\" becomes \"isnt\")\n",
    "    text = [re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\").sub(\" \", line) for line in text] # replace \"<br /><br />\", etc. by \" \" with space (e.g., \"<br /><br />\" becomes \" \")\n",
    "    text = [re.compile(\"[^\\x00-\\x7f]\").sub(\"\", line) for line in text] # replace all ascii key codes \\x96, \\x97, etc. by \"\"\n",
    "    return text\n",
    "\n",
    "train_clean = clean_texts(train)\n",
    "test_clean = clean_texts(test)\n",
    "print(len(train_clean), len(test_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print train data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### - Remove stop words\n",
    "- The text also contains stop words (is, a, it, as, some, etc.). Let's remove them.\n",
    "    - By doing so, you can focus more on \"meaningful\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    remain_words = []\n",
    "    for item in text:\n",
    "        remain_words.append(' '.join([word for word in item.split() if word not in stop_words]))\n",
    "    return remain_words\n",
    "\n",
    "train_clean_no_stop = remove_stop_words(train_clean)\n",
    "test_clean_no_stop = remove_stop_words(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print train data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### - Normalization\n",
    "- Finally, we normalize text. For example:\n",
    "    - am, is, are -> be\n",
    "    - play, plays, playing, played -> play\n",
    "- There are two major ways to normalize text:\n",
    "    - Stemming: Simply return stems without knowledge of the context (e.g. connections, connected, connecting, connection -> connect).\n",
    "    - Lemmatization: Iovolve a vocabulary and morphological analysis of words (e.g. better -> good, meeting -> meeting or meet). Return the base or dictionary form of a word (= the lemma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stemming\n",
    "- There are several types of the stemmer (see e.g. [this webpage](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)):\n",
    "    - Porter stemmer\n",
    "    - Lancaster stemmer\n",
    "    - etc.\n",
    "- We will use Porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_stemmed_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [' '.join([stemmer.stem(word) for word in item.split()]) for item in text]\n",
    "    return stemmed_text\n",
    "\n",
    "train_clean_stemmed = get_stemmed_text(train_clean)\n",
    "test_clean_stemmed = get_stemmed_text(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lemmatization\n",
    "- We will use WordNet Lemmatizer. \n",
    "    - It uses the WordNet Database to lookup lemmas of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_lemmatized_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [' '.join([lemmatizer.lemmatize(word) for word in item.split()]) for item in text]\n",
    "    return lemmatized_text\n",
    "\n",
    "train_clean_lemmatized = get_lemmatized_text(train_clean)\n",
    "test_clean_lemmatized = get_lemmatized_text(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Vectorization\n",
    "- Let's convert text to numeric values.... How?\n",
    "- There are several methods:\n",
    "    - Bag-of-Words (BOW)\n",
    "    - Word Counts\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - N-grams\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's use the lemmatized version for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_clean_lemmatized # or train_clean_stemmed\n",
    "test_data = test_clean_lemmatized # or test_clean_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print the first review in the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag-of-Words (BOW)\n",
    "- Make a review-by-word matrix which takes 1 if a word appears in a review, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=True)\n",
    "vec.fit(train_data) # set a vocabulary dictionary of all words in the train data\n",
    "X = vec.transform(train_data) # transform train data to a review-by-word sparce matrix using the dictionary (takes either 0 or 1 with many zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's check the size of vocabulary and all the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(len(vec.vocabulary_)))\n",
    "print('Vocabulary content: {}'.format(vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's check the content of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Delete ``df_X`` for now because it consumes a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word Counts\n",
    "- The method is similar to BOW but takes into account the frequency of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=False)\n",
    "vec.fit(train_data)\n",
    "X = vec.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Check the content of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df_X['high'][df_X['high']!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_X['high'], bins=8, align='mid', range=(1,8), alpha=0.3, color='r')\n",
    "plt.title('Frequency of \"high\" per review') \n",
    "plt.ylabel('reviews')\n",
    "plt.xlabel('times')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Delete ``df_X`` for now because it consumes a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "del df_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- The method takes into account the frequency of words in a document and in all documents.\n",
    "- The formula: TF * IDF, where\n",
    "    - TF: # of word `X` in review i / # of all words in review i\n",
    "    - IDF: log(# of reviews / # of reviews that contains word `X`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "vec.fit(train_data)\n",
    "X = vec.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Check the content of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df_X['high'][df_X['high']!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_X['high'], bins=8, align='mid', range=(0.001,1), alpha=0.3, color='r')\n",
    "plt.title('TF-IDF of \"high\" per review') \n",
    "plt.ylabel('reviews')\n",
    "plt.xlabel('TF-IDF')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Delete ``df_X`` for now because it consumes a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "del df_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### N-grams\n",
    "- All the above three methods consider only single words.\n",
    "- An alternative approach is to consider multiple consecutive words.\n",
    "- Let's collect all two consecutive words. They are called bi-grams.\n",
    "    - Increasing N does not necessarily improve accuracy. Also, computation takes a longer time for a larger N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=True, ngram_range=(1,2))\n",
    "vec.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's print data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(len(vec.vocabulary_)))\n",
    "print('bi-grams containing \"good\": {}'.format([key for key, value in vec.vocabulary_.items() if \"good\" in key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Sentiment Analysis\n",
    "- We train a machine such that it can predict the sentiment of movie reviews with high precision.\n",
    "- There are several types of learning algorithms:\n",
    "    - Supervised learning: Labeled train data are needed.\n",
    "    - Unsupervised learning: Learn from unlabeled test data.\n",
    "    - Others (semi-supervised learning, reinforcement learning, etc.)\n",
    "- We will use supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are several models:\n",
    "    - Linear models (e.g., OLS, logistic regression)\n",
    "    - Support Vector Machines\n",
    "    - Decision Trees\n",
    "    - Ensemble methods (e.g., Random Forest)\n",
    "    - Neural Networks\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "- Let's start by using a simple model: Logistic regression.\n",
    "- There are so many words in the data. \n",
    "    - We want to consider only a few words that are important for prediction.\n",
    "- Using regularization, we punish words that are not important for prediction.\n",
    "- Let's find the best \"c\", which is the inverse of regularization strength.\n",
    "    - A smaller value means a stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's vectorize text using BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=True)\n",
    "vec.fit(train_data) \n",
    "X = vec.transform(train_data) \n",
    "X_test = vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Next, randomly split train data into two groups and predict one from the other.\n",
    "- Find the best \"c\" that yields the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "target = [1 if i < 12500 else 0 for i in range(25000)] # make a list, which takes 1 for pos and 0 for neg\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, target, train_size=0.75, random_state=12345) # split \"X\" and \"traget\" into 2 groups, train_size means how many you want to allocate to the train group\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]: \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_tr, y_tr)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_te, lr.predict(X_te))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- It seems that the best \"c\" is 0.05.\n",
    "    - This means that only a few words are important for prediction.\n",
    "- Let's see the accuracy of our machine using the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.05)\n",
    "lr.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(target, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's compare the correct answer and the machine's prediction for a specific review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'text':test_data, 'pred':lr.predict(X_test), 'orig':target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "row = 2 # choose a specific review\n",
    "print(out.loc[row,'text']); print('predicted =', out.loc[row,'pred']); print('correct =', out.loc[row,'orig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's play with the data by changing reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_clean_mod = test_clean\n",
    "test_clean_mod[0] = 'i do not like this movie'\n",
    "test_clean_mod[1] = 'i love this movie'\n",
    "X_test2 = vec.transform(test_clean_mod)\n",
    "out2 = pd.DataFrame({'text':test_clean_mod, 'pred':lr.predict(X_test2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(out2.loc[0,'text']); print('predicted =', out2.loc[0,'pred'])\n",
    "print(out2.loc[1,'text']); print('predicted =', out2.loc[1,'pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Support Vector Machines\n",
    "- Linear Support Vector Machines find the \"maximum-margin\" hyperplane that separates data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    linear = LinearSVC(C=c)\n",
    "    linear.fit(X_tr, y_tr)\n",
    "    print(\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_te, linear.predict(X_te))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- It seems that the best \"c\" is 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "linear = LinearSVC(C=0.01)\n",
    "linear.fit(X, target)\n",
    "print(\"Final Accuracy: %s\" % accuracy_score(target, linear.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Trees\n",
    "- A tree of inputs is made from train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=12345, max_depth=10) \n",
    "dt.fit(X, target)\n",
    "print(\"Final Accuracy: %s\" % accuracy_score(target, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forests\n",
    "- Each tree is built from a randomly drawn sample from train data with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=12345, max_depth=10)\n",
    "rf.fit(X, target)\n",
    "print(\"Final Accuracy: %s\" % accuracy_score(target, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Networks\n",
    "- Neural Networks (or Multi-layer Perceptron) mimic neurons.\n",
    "- A network has hidden layers between the input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', random_state=12345, max_iter=5)\n",
    "clf.fit(X, target)\n",
    "print(\"Final Accuracy: %s\" % accuracy_score(target, clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
