{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text as Data using Python\n",
    "By Shuhei Kitamura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Nowadays, many kinds of text data are available online.\n",
    "    - E.g., news articles, customer reviews, tweets, etc.\n",
    "- Possible analysis using such data includes:\n",
    "    - Sentiment analysis (pos/neg), conflict/hate speech detection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's evaluate the sentiment (pos/neg) of user reviews in [IMDb (Internet Movie Database)](https://www.imdb.com/)!\n",
    "    - You can possibly apply the same method to analyze the sentiment of e.g. Amazon customer reviews.\n",
    "- In particular, we:\n",
    "    1. Prepare data for analysis using techniques in natural language processing (NLP)\n",
    "    2. Convert texts to a matrix\n",
    "    3. Conduct a sentiment analysis using machine learning\n",
    "- The following packages will be mainly used:\n",
    "    - nltk\n",
    "    - scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline<a id='top'></a>\n",
    "1. [Importing data](#sec1)\n",
    "2. [Cleaning data](#sec2)\n",
    "    1. [Removing unnecessary items](#sec2_1)\n",
    "    2. [Normalization](#sec2_2)\n",
    "    3. [Removing stop words](#sec2_3)    \n",
    "3. [Vectorization](#sec3)\n",
    "    1. [Bag-of-Words](#sec3_1)\n",
    "    2. [Word Counts](#sec3_2)\n",
    "    3. [Term Frequency-Inverse Document Frequency (TF-IDF)](#sec3_3)\n",
    "    4. [N-grams](#sec3_4)\n",
    "4. [Sentiment Analysis](#sec4)\n",
    "    1. [Logistic Regressions](#sec4_1)\n",
    "    2. [Support Vector Machines](#sec4_2)\n",
    "    3. [Decision Trees](#sec4_3)\n",
    "    4. [Random Forests](#sec4_4)\n",
    "    5. [Neural Networks](#sec4_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import packages and modules\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk; nltk.download('stopwords'); nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# os.chdir('...') # set the directory if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Importing data<a id='sec1'></a>\n",
    "- Let's import data as lists.\n",
    "    - ``full_train.txt``: train data.\n",
    "    - ``full_test.txt``: test data\n",
    "- We will train the machine using ``full_train.txt`` and then apply it to ``full_test.txt``.\n",
    "- All data were taken from [this website](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train = [] # import data as a list\n",
    "with open('full_train.txt', encoding=\"utf-8\") as file: \n",
    "    for line in file:\n",
    "        train.append(line.strip())\n",
    "        \n",
    "test = [] # import data as a list\n",
    "with open('full_test.txt', encoding=\"utf-8\") as file: \n",
    "    for line in file:\n",
    "        test.append(line.strip())\n",
    "        \n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print the first item in `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Cleaning data<a id='sec2'></a>\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A. Removing unnecessary items<a id='sec2_1'></a>\n",
    "- The text contains many unnecessary items such as punctuation marks (., :, ;, !, ?), parentheses, etc.\n",
    "- We will remove them first.\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def clean_texts(text): # define a function \n",
    "    t = [re.sub(\"[*.;:!\\'?,\\\"()\\[\\]]\", \"\", line.lower()) for line in text] # make all text be lowercases, then remove punctuation marks, parantheses, etc. (e.g., \"isn't\" becomes \"isnt\")\n",
    "    t = [re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\", \" \", line) for line in t] # replace \"<br /><br />\", etc. with \" \" with a space (e.g., \"<br /><br />\" becomes \" \")\n",
    "    t = [re.sub(\"[^\\x00-\\x7f]\", \"\", line) for line in t] # remove all ascii key codes \\x96, \\x97, etc. see, e.g., https://www.codetable.net/asciikeycodes\n",
    "    return t\n",
    "\n",
    "train_clean = clean_texts(train) # apply the function\n",
    "test_clean = clean_texts(test)\n",
    "\n",
    "print(len(train_clean), len(test_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print the first item in `train_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### B. Normalization<a id='sec2_2'></a>\n",
    "- Next, we normalize text. For example:\n",
    "    - am, is, are -> be\n",
    "    - play, plays, playing, played -> play\n",
    "- There are two major ways to normalize text:\n",
    "    - Stemming: Simply return stems without knowledge of the context (e.g. connections, connected, connecting, connection -> connect).\n",
    "    - Lemmatization: Iovolve a vocabulary and morphological analysis of words (e.g. better -> good, meeting -> meeting or meet). Return the base or dictionary form of a word (= the lemma).\n",
    "        \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stemming\n",
    "- There are several types of the stemmer (see e.g. [this webpage](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)):\n",
    "    - Porter stemmer\n",
    "    - Lancaster stemmer\n",
    "    - etc.\n",
    "- We will use Porter stemmer.\n",
    "- Run the following code. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_stemmed_text(text): \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [' '.join([stemmer.stem(word) for word in line.split()]) for line in text]\n",
    "    return stemmed_text\n",
    "\n",
    "train_clean_stemmed = get_stemmed_text(train_clean)\n",
    "test_clean_stemmed = get_stemmed_text(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print the first item in `train_clean_stemmed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lemmatization\n",
    "- We will use WordNet Lemmatizer. \n",
    "    - It uses the [WordNet Database](https://wordnet.princeton.edu/) to lookup lemmas of words.\n",
    "- Run the following code. This process also takes time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_lemmatized_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [' '.join([lemmatizer.lemmatize(word) for word in line.split()]) for line in text]\n",
    "    return lemmatized_text\n",
    "\n",
    "train_clean_lemmatized = get_lemmatized_text(train_clean)\n",
    "test_clean_lemmatized = get_lemmatized_text(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print the first item in `train_clean_lemmatized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### C. Removing stop words<a id='sec2_3'></a>\n",
    "- The text still contains stop words (is, a, it, as, some, etc.). Let's remove them.\n",
    "    - By doing so, you can focus more on \"meaningful\" words.\n",
    "        \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') # define stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text): # make a function\n",
    "    remain_words = []\n",
    "    for line in text:\n",
    "        remain_words.append(' '.join([word for word in line.split() if word not in stop_words])) # join all words with ' ' inbetween\n",
    "    return remain_words\n",
    "\n",
    "train_clean_no_stop = remove_stop_words(train_clean_lemmatized) # apply the function  # alternatively, use train_clean_stemmed\n",
    "test_clean_no_stop = remove_stop_words(test_clean_lemmatized) # alternatively, use test_clean_stemmed\n",
    "\n",
    "print(len(train_clean_no_stop), len(test_clean_no_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Print the first item in `train_clean_no_stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Vectorization<a id='sec3'></a>\n",
    "- Let's convert text to numeric values.... How?\n",
    "- There are several methods:\n",
    "    - Bag-of-Words (BOW)\n",
    "    - Word Counts\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - N-grams\n",
    "    - etc.\n",
    "        \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's use the lemmatized version for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_clean_no_stop\n",
    "test_data = test_clean_no_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A. Bag-of-Words (BOW)<a id='sec3_1'></a>\n",
    "- Make a review-by-word matrix which takes 1 if a word appears in a review, and 0 otherwise.\n",
    "        \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=True)\n",
    "vec.fit(train_data) # learn a vocabulary dictionary of all words in the train data\n",
    "X = vec.transform(train_data) # transform train data to a review x word sparse matrix using the dictionary (takes either 0 or 1 with many zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's check the size of vocabulary and all the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(len(vec.vocabulary_)))\n",
    "print('Vocabulary content: {}'.format(vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's check the content of the matrix.\n",
    "    - If you get an error, that's fine. It says that your computer does not have enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Delete ``df_X`` for now because it consumes a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "del df_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### B. Word Counts<a id='sec3_2'></a>\n",
    "- The method is similar to BOW but takes into account the frequency of words.\n",
    "        \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=False)\n",
    "vec.fit(train_data)\n",
    "X = vec.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Check the content of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df_X['high'][df_X['high']>3]) # print the frequency of 'high' in reviews that contain at least 3 'high's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_X['high'], bins=8, align='mid', range=(1,8), alpha=0.3, color='r') # print the frequency of 'high' per review\n",
    "plt.title('Frequency of \"high\" per review') \n",
    "plt.ylabel('reviews')\n",
    "plt.xlabel('times')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Delete ``df_X`` for now because it consumes a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "del df_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### C. Term Frequency-Inverse Document Frequency (TF-IDF)<a id='sec3_3'></a>\n",
    "- The method takes into account the frequency of words in a document and in all documents.\n",
    "- Still, a higher number means that the word is frequently used.\n",
    "- The formula: TF * IDF, where\n",
    "    - TF: # of word `X` in review i / # of all words in review i\n",
    "    - IDF: log(# of reviews / # of reviews that contains word `X`)\n",
    "\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "vec.fit(train_data)\n",
    "X = vec.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Check the content of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df_X['high'][df_X['high']>0.2]) # print the frequency of 'high' in reviews where TF-IDF of 'high' is larger than 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_X['high'], bins=8, align='mid', range=(0.001,1), alpha=0.3, color='r') # print TF-IDF of 'high' per review\n",
    "plt.title('TF-IDF of \"high\" per review') \n",
    "plt.ylabel('reviews')\n",
    "plt.xlabel('TF-IDF')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Delete ``df_X`` for now because it consumes a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "del df_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### D. N-grams<a id='sec3_4'></a>\n",
    "- All the above three methods consider only single words.\n",
    "- An alternative approach is to consider multiple consecutive words.\n",
    "- Let's collect all two consecutive words. They are called bi-grams.\n",
    "    - Increasing N does not necessarily improve accuracy. Also, computation takes a longer time for a larger N.\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=True, ngram_range=(1,2))\n",
    "vec.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's print data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(len(vec.vocabulary_)))\n",
    "print('bi-grams containing \"good\": {}'.format([key for key, value in vec.vocabulary_.items() if \"good\" in key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Sentiment Analysis<a id='sec4'></a>\n",
    "- We train a machine such that it can predict the sentiment (pos/neg) of movie reviews with high precision.\n",
    "- There are several types of learning algorithms:\n",
    "    - Supervised learning: Labeled train data are needed.\n",
    "    - Unsupervised learning: Learn from unlabeled test data.\n",
    "    - Others (semi-supervised learning, reinforcement learning, etc.)\n",
    "- We will use supervised learning.\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are several models:\n",
    "    - Linear models (e.g., OLS, logistic regression)\n",
    "    - Support Vector Machines\n",
    "    - Decision Trees\n",
    "    - Ensemble methods (e.g., Random Forest)\n",
    "    - Neural Networks\n",
    "    - etc.\n",
    "- [Cheat sheets](https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463) are also available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A. Logistic Regressions<a id='sec4_1'></a>\n",
    "- Let's start by using a simple model: Logistic regression.\n",
    "- There are so many words in the data. \n",
    "- We want to consider only a few words that are important for prediction.\n",
    "- Using **regularization**, we punish words that are not important for prediction.\n",
    "- Let's find the best \"c\", which is the inverse of regularization strength.\n",
    "    - A smaller value means a stronger regularization.\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- First, vectorize text using BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=True)\n",
    "vec.fit(train_data) \n",
    "X = vec.transform(train_data) # a 250000 x 84063 scipy sparce matrix\n",
    "X_test = vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Next, randomly split train data into two groups and predict one from the other.\n",
    "    - In the below example, we use `X_train_sub` and `y_train_sub` to train the machine.\n",
    "    - Then predict `y_pred_sub` by using `X_test_sub`.\n",
    "    - Finally, compare `y_pred_sub` (predicted values) with `y_test_sub` (true values).\n",
    "- Find the best \"c\" that yields the highest accuracy.\n",
    "    - You may get a warning message but that's fine for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y = [1 if i < 12500 else 0 for i in range(25000)] # make a list, which takes 1 for pos (first 12500 obs) and 0 for neg (next 12500 obs)\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X, y, train_size=0.75, random_state=12345) # split X and y into 2 groups, train_size means how many you want to allocate to the train group\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]: \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train_sub, y_train_sub) # train a machine\n",
    "    y_pred = lr.predict(X_test_sub) # predict values using X_test_sub\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test_sub, y_pred))) # compare true values (y_test_sub) with predicted values (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- It seems that the best \"c\" is 0.05.\n",
    "    - This means that only a few words are important for prediction.\n",
    "- Let's see the accuracy of our machine using the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.05)\n",
    "lr.fit(X, y)\n",
    "y_pred = lr.predict(X_test)\n",
    "print (\"Accuracy is: %s\" % accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's compare the correct answer and the machine's prediction for a specific review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'text':test_data, 'pred':y_pred, 'orig':y}) # make a dataframe\n",
    "row = 2 # choose a third review\n",
    "print(out.loc[row, 'text']); print('predicted =', out.loc[row, 'pred']); print('correct =', out.loc[row, 'orig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's play with the data by manually changing reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_clean_mod = test_clean\n",
    "test_clean_mod[0] = 'i do not like this movie' # change the first review\n",
    "test_clean_mod[1] = 'i love this movie' # change the second review\n",
    "X_test_mod = vec.transform(test_clean_mod) # make a sparse matrix\n",
    "y_mod_pred = lr.predict(X_test_mod) # predict values\n",
    "out_mod = pd.DataFrame({'text':test_clean_mod, 'pred':y_mod_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(out_mod.loc[0, 'text']); print('predicted =', out_mod.loc[0, 'pred'])\n",
    "print(out_mod.loc[1, 'text']); print('predicted =', out_mod.loc[1, 'pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### B. Support Vector Machines<a id='sec4_2'></a>\n",
    "- Linear Support Vector Machines find the maximum-margin hyperplane that separates data points ([web](https://scikit-learn.org/stable/modules/svm.html)).\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    linear = LinearSVC(C=c)\n",
    "    linear.fit(X_train_sub, y_train_sub)\n",
    "    y_pred = linear.predict(X_test_sub)\n",
    "    print(\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test_sub, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- It seems that the best \"c\" is 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "linear = LinearSVC(C=0.01)\n",
    "linear.fit(X, y)\n",
    "y_pred = linear.predict(X_test)\n",
    "print(\"Accuracy is: %s\" % accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### C. Decision Trees<a id='sec4_3'></a>\n",
    "- The machine learns simple decision rules inferred from the train data ([web](https://scikit-learn.org/stable/modules/tree.html#tree)).\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=12345, max_depth=10) \n",
    "dt.fit(X, y)\n",
    "y_pred = dt.predict(X_test)\n",
    "print(\"Accuracy is: %s\" % accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### D. Random Forests<a id='sec4_4'></a>\n",
    "- Each tree is built from a randomly drawn sample from train data with replacement ([web](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)).\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=12345, max_depth=10)\n",
    "rf.fit(X, y)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy is: %s\" % accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### E. Neural Networks<a id='sec4_5'></a>\n",
    "- Neural Networks (or Multi-layer Perceptron) mimic neurons ([web](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)).\n",
    "- A network has hidden layers between an input and output. A neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation, followed by a function to produce an output. Weights are updated for each iteration.\n",
    "    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', random_state=12345, max_iter=5) # default max_iter is 200 but it takes time to run the code\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy is: %s\" % accuracy_score(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
